import torch
from torch import nn
from transformers import PreTrainedModel, Trainer, TrainingArguments, BertConfig, BertModel
from datasets import Dataset

# 1. å®šä¹‰è‡ªå®šä¹‰çš„Transformeræ¨¡å‹ï¼Œç”¨äºåºåˆ—é¢„æµ‹
class ClickPredictionModel(PreTrainedModel):
    config_class = BertConfig

    def __init__(self, config):
        super().__init__(config)
        self.encoder = BertModel(config).encoder  # ç›´æ¥ä½¿ç”¨Transformer encoderå±‚
        self.classifier = nn.Linear(config.hidden_size, config.hidden_size)  # é¢„æµ‹ä¸‹ä¸€ä¸ªåµŒå…¥

        self.init_weights()

    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None):
        # å°†è¾“å…¥æ•°æ®å½¢çŠ¶è°ƒæ•´ä¸º (batch_size, seq_length, hidden_size)
        batch_size, seq_length, hidden_size = input_ids.shape

        # è°ƒç”¨encoderéƒ¨åˆ†çš„forwardå‡½æ•°
        outputs = self.encoder(
            hidden_states=input_ids,
            attention_mask=attention_mask
        )

        sequence_output = outputs[0]  # shape: (batch_size, seq_length, hidden_size)
        logits = self.classifier(sequence_output)  # shape: (batch_size, seq_length, hidden_size)

        loss = None
        if labels is not None:
            loss_fct = nn.MSELoss()
            loss = loss_fct(logits.view(-1, hidden_size), labels.view(-1, hidden_size))

        return (loss, logits) if loss is not None else logits

# 2. åŠ è½½å’Œå‡†å¤‡æ•°æ®é›†
def create_dataset_from_embeddings(embedding_file):
    # åŠ è½½åµŒå…¥æ•°æ®
    embeddings = torch.load(embedding_file)

    # åˆ›å»ºè¾“å…¥å’Œæ ‡ç­¾
    input_ids = embeddings[:, :-1, :]  # æ‰€æœ‰åºåˆ—ï¼Œå»æ‰æœ€åä¸€ä¸ªä½œä¸ºè¾“å…¥
    labels = embeddings[:, 1:, :]  # æ‰€æœ‰åºåˆ—ï¼Œå»æ‰ç¬¬ä¸€ä¸ªä½œä¸ºæ ‡ç­¾

    return Dataset.from_dict({'input_ids': input_ids, 'labels': labels})

embedding_file = 'user_history_embedding.pt'
dataset = create_dataset_from_embeddings(embedding_file)

# 3. è®¾ç½®è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir='./results',          # output directory
    evaluation_strategy="epoch",     # evaluate each `epoch`
    learning_rate=2e-5,              # learning rate
    per_device_train_batch_size=2,   # batch size for training
    per_device_eval_batch_size=2,    # batch size for evaluation
    num_train_epochs=3,              # total number of training epochs
    weight_decay=0.01,               # strength of weight decay
)

# 4. åˆå§‹åŒ–æ¨¡å‹å’ŒTrainer
config = BertConfig(vocab_size=1, hidden_size=108, num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)
model = ClickPredictionModel(config)

trainer = Trainer(
    model=model,                         # the instantiated ğŸ¤— Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=dataset,               # training dataset
    eval_dataset=dataset                 # evaluation dataset (for simplicity, use the same dataset)
)

# 5. è®­ç»ƒæ¨¡å‹
trainer.train()
