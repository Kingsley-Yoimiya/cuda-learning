{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPvJz2Nvrjyox7Ui8XzMzyL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kingsley-Yoimiya/cuda-learning/blob/main/simple_practice/CUDA_TEST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# add nvcc support for jupyter\n",
        "!pip install nvcc4jupyter\n",
        "\n",
        "%load_ext nvcc4jupyter\n",
        "\n",
        "!nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ie07XVaXVglG",
        "outputId": "b06a421f-08c9-48ea-a4cc-6a2a1dd8ee63"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nvcc4jupyter in /usr/local/lib/python3.10/dist-packages (1.2.1)\n",
            "Detected platform \"Colab\". Running its setup...\n",
            "Source files will be saved in \"/tmp/tmpjxzhldmf\".\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "\n",
        "#include <iostream>\n",
        "#include <random>\n",
        "#include <chrono>\n",
        "#include <cuda_runtime.h>\n",
        "#include <iostream>\n",
        "#include <ctime>\n",
        "#include <cuda.h>\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "#define BLOCK_SIZE 256\n",
        "\n",
        "__global__ void reduce_sum_kernel(const float* input_vecs, size_t n, size_t dim, float* output_vec) {\n",
        "    size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if(idx < n * dim) {\n",
        "        atomicAdd(&output_vec[idx % dim], input_vecs[idx]);\n",
        "    }\n",
        "}\n",
        "\n",
        "void reduce_sum(const float* input_vecs, size_t n, size_t dim, float* output_vec) {\n",
        "    float* cu_input_vecs;\n",
        "    float* cu_output_vecs;\n",
        "    size_t input_size = n * dim * sizeof(float), output_size = dim * sizeof(float);\n",
        "    cudaMalloc((void**) &cu_input_vecs, input_size);\n",
        "    cudaMalloc((void**) &cu_output_vecs, output_size);\n",
        "    cudaMemcpy(cu_input_vecs, input_vecs, input_size, cudaMemcpyHostToDevice);\n",
        "    cudaMemset(cu_output_vecs, 0, output_size);\n",
        "    size_t grid_size = (n * dim + BLOCK_SIZE - 1) / BLOCK_SIZE;\n",
        "    reduce_sum_kernel <<< grid_size, BLOCK_SIZE >>>(cu_input_vecs, n, dim, cu_output_vecs);\n",
        "    cudaDeviceSynchronize();\n",
        "    cudaMemcpy(output_vec, cu_output_vecs, output_size, cudaMemcpyDeviceToHost);\n",
        "    cudaFree(cu_input_vecs);\n",
        "    cudaFree(cu_output_vecs);\n",
        "}\n",
        "\n",
        "const long long N = 1e9;\n",
        "const int T = 1000;\n",
        "\n",
        "uniform_real_distribution<float> u(0, 1);\n",
        "mt19937 rnd(chrono::system_clock::now().time_since_epoch().count());\n",
        "\n",
        "int main() {\n",
        "    float* input_vecs = new float[N];\n",
        "    float* output_vec = new float[T];\n",
        "    float* correct_vec = new float[T];\n",
        "    for(int i = 0; i < N; i++) {\n",
        "        input_vecs[i] = u(rnd);\n",
        "    }\n",
        "\n",
        "    cerr << \"GENERATE OK!\" << endl;\n",
        "    double st = clock();\n",
        "    reduce_sum(input_vecs, N / T, T, output_vec);\n",
        "    double ed = clock();\n",
        "    std::cout << (ed - st) / CLOCKS_PER_SEC << std::endl;\n",
        "\n",
        "    st = clock();\n",
        "\n",
        "    for(int i = 0; i < T; i++) {\n",
        "        correct_vec[i] = 0;\n",
        "    }\n",
        "    for(int i = 0; i < N; i++) {\n",
        "        correct_vec[i % T] += input_vecs[i];\n",
        "    }\n",
        "\n",
        "    ed = clock();\n",
        "    std::cout << (ed - st) / CLOCKS_PER_SEC << std::endl;\n",
        "\n",
        "    for(int i = 0; i < T; i++) {\n",
        "        if(abs(correct_vec[i] - output_vec[i]) > 1) {\n",
        "            std::cout << correct_vec[i] << \" \" << output_vec[i] << \" ERROR!\" << std::endl;\n",
        "            break;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    std::cout << output_vec[0] << std::endl;\n",
        "    delete[] input_vecs;\n",
        "    delete[] output_vec;\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WW1lsLB3MMya",
        "outputId": "bc57ecda-20da-4fb7-f72d-feacaf805188"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATE OK!\n",
            "0.991417\n",
            "3.25612\n",
            "500030\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "\n",
        "#include <iostream>\n",
        "#include <random>\n",
        "#include <chrono>\n",
        "#include <cuda_runtime.h>\n",
        "#include <iostream>\n",
        "#include <ctime>\n",
        "#include <cuda.h>\n",
        "#include <assert.h>\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "#define GRID_SIZE 128\n",
        "#define BLOCK_SIZE 256\n",
        "#define cudaCheckError() {                                                      \\\n",
        "    cudaError_t err = cudaGetLastError();                                       \\\n",
        "    if (err != cudaSuccess) {                                                   \\\n",
        "        std::cerr << \"CUDA error: \" << cudaGetErrorString(err)                  \\\n",
        "                  << \" at \" << __FILE__ << \":\" << __LINE__ << std::endl;        \\\n",
        "        exit(EXIT_FAILURE);                                                     \\\n",
        "    }                                                                           \\\n",
        "}\n",
        "#define cudaCheckErrorSync() {                                                  \\\n",
        "    cudaDeviceSynchronize();                                                    \\\n",
        "    cudaError_t err = cudaGetLastError();                                       \\\n",
        "    if (err != cudaSuccess) {                                                   \\\n",
        "        std::cerr << \"CUDA error: \" << cudaGetErrorString(err)                  \\\n",
        "                  << \" at \" << __FILE__ << \":\" << __LINE__ << std::endl;        \\\n",
        "        exit(EXIT_FAILURE);                                                     \\\n",
        "    }                                                                           \\\n",
        "}\n",
        "\n",
        "#define cudaCheckErrorSync() {}\n",
        "#define cudaCheckError() {}\n",
        "\n",
        "__global__ void matmul_kernel(const float* A, const float* B, size_t n, size_t m, size_t k, float* output) {\n",
        "    size_t idi = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    size_t idj = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    if(idi < n && idj < k) {\n",
        "        float res = 0;\n",
        "        for(int idk = 0; idk < m; idk++) {\n",
        "            res += A[idi * m + idk] * B[idk * k + idj];\n",
        "        }\n",
        "        output[idi * k + idj] = res;\n",
        "    }\n",
        "}\n",
        "\n",
        "void matmul(const float* A, const float* B, size_t n, size_t m, size_t k, float* output) {\n",
        "    float* cu_A;\n",
        "    float* cu_B;\n",
        "    float* cu_output;\n",
        "    size_t A_size = n * m * sizeof(float), B_size = m * k * sizeof(float), out_size = n * k * sizeof(float);\n",
        "    cudaMalloc((void**) &cu_A, A_size);\n",
        "    cudaCheckError();\n",
        "    cudaMalloc((void**) &cu_B, B_size);\n",
        "    cudaMalloc((void**) &cu_output, out_size);\n",
        "    cudaCheckError();\n",
        "    cudaMemcpy(cu_A, A, A_size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(cu_B, B, B_size, cudaMemcpyHostToDevice);\n",
        "    cudaCheckError();\n",
        "    cudaMemset(cu_output, 0, out_size);\n",
        "    dim3 grid(GRID_SIZE, GRID_SIZE);\n",
        "    dim3 block((n + GRID_SIZE - 1) / GRID_SIZE, (k + GRID_SIZE - 1) / GRID_SIZE);\n",
        "    cerr << (n + GRID_SIZE - 1) / GRID_SIZE << \" \" << (k + GRID_SIZE - 1) / GRID_SIZE << endl;\n",
        "    matmul_kernel <<< grid, block >>>(cu_A, cu_B, n, m, k, cu_output);\n",
        "    cudaCheckErrorSync();\n",
        "\n",
        "    cudaDeviceSynchronize();\n",
        "    cudaMemcpy(output, cu_output, out_size, cudaMemcpyDeviceToHost);\n",
        "    cudaCheckError();\n",
        "    cudaFree(cu_A);\n",
        "    cudaFree(cu_B);\n",
        "    cudaFree(cu_output);\n",
        "}\n",
        "\n",
        "const int N = 2000, M = 2000, K = 2000;\n",
        "const int T = 100;\n",
        "\n",
        "uniform_real_distribution<float> u(0, 1);\n",
        "mt19937 rnd(chrono::system_clock::now().time_since_epoch().count());\n",
        "\n",
        "int main() {\n",
        "    float* A = new float[N * M];\n",
        "    float* B = new float[M * K];\n",
        "    float* C = new float[N * K];\n",
        "    float* D = new float[N * K];\n",
        "    for(int i = 0; i < N * M; i++) {\n",
        "        A[i] = u(rnd);\n",
        "    }\n",
        "    for(int i = 0; i < M * K; i++) {\n",
        "        B[i] = u(rnd);\n",
        "    }\n",
        "    cerr << \"GENERATE OK!\" << endl;\n",
        "\n",
        "    double st = clock();\n",
        "    matmul(A, B, N, M, K, C);\n",
        "    double ed = clock();\n",
        "    std::cout << (ed - st) / CLOCKS_PER_SEC << std::endl;\n",
        "\n",
        "    st = clock();\n",
        "    for(int i = 0; i < N * K; i++) {\n",
        "        D[i] = 0;\n",
        "    }\n",
        "    for(int i = 0; i < N; i++) {\n",
        "       for(int j = 0; j < M; j++) {\n",
        "            for(int k = 0; k < K; k++) {\n",
        "                D[i * K + k] += A[i * M + j] * B[j * K + k];\n",
        "            }\n",
        "       }\n",
        "    }\n",
        "\n",
        "    ed = clock();\n",
        "    std::cout << (ed - st) / CLOCKS_PER_SEC << std::endl;\n",
        "\n",
        "    for(int i = 0; i < N * K; i++) {\n",
        "        if(fabs(C[i] - D[i]) > 1e-2) {\n",
        "            std::cout << C[i] << \" \" << D[i] << \" ERROR!\" << std::endl;\n",
        "            break;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    std::cout << \"RESULT: \" << C[0] << std::endl;\n",
        "    delete[] A;\n",
        "    delete[] B;\n",
        "    delete[] C;\n",
        "    delete[] D;\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rnuJHlWXzZS",
        "outputId": "83fbf60c-6422-44a2-d85b-fc4365018d01"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATE OK!\n",
            "16 16\n",
            "0.374476\n",
            "32.6915\n",
            "RESULT: 526.606\n",
            "\n"
          ]
        }
      ]
    }
  ]
}